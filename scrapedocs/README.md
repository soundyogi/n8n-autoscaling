# Documentation Scraper

A smart Node.js web scraper that extracts documentation from any website and converts it to clean markdown files. Perfect for creating local copies of API docs, guides, and technical documentation.

## Features

- ğŸ¯ **Smart Link Filtering** - Automatically finds relevant documentation links
- ğŸ“ **HTML to Markdown** - Preserves formatting (headers, code blocks, links, lists)
- ğŸ“ **Organized Output** - Creates folders named after websites with URL-based filenames
- ğŸš€ **Concurrent Processing** - Efficiently scrapes multiple pages simultaneously
- ğŸ›¡ï¸ **Error Resilient** - Continues scraping even if individual pages fail
- ğŸŒ **Universal** - Works with any documentation site (GitHub Docs, API docs, etc.)

## Quick Start

```bash
# Install dependencies
npm install axios cheerio puppeteer --save

sudo apt-get update && sudo apt-get install -y wget gnupg ca-certificates procps libxss1 libnss3 libatk-bridge2.0-0 libgtk-3-0 libgbm-dev libasound2

# Scrape any documentation site
node scrape.js https://docs.github.com

# Results in organized markdown files:
# docs_github_com/
#   â”œâ”€â”€ en_get-started_quickstart.md
#   â”œâ”€â”€ en_actions_learn-github-actions.md
#   â””â”€â”€ api_rest_overview.md
```

## Installation

1. Clone or download the scraper
2. Install dependencies:
   ```bash
   npm install axios cheerio
   ```

## Usage

### Command Line
```bash
node scrape.js <URL>
```

### Programmatic Usage
```javascript
import * as scrape from './scrape.js';

// Basic usage
await scrape.run('https://docs.example.com');

// With custom options
await scrape.run('https://docs.example.com', {
  includePatterns: ['/docs', '/api'],    // Only scrape these paths
  excludePatterns: ['/login', '/auth'],  // Skip these paths
  contentSelector: 'article, main',     // Where to find content
  maxLinks: 20,                          // Limit number of pages
  singleFile: true,                      // Combine into one file
  concurrency: 3                         // Concurrent requests
});
```

## Configuration Options

| Option | Default | Description |
|--------|---------|-------------|
| `includePatterns` | `['/docs', '/api', '/reference']` | URL patterns to include |
| `excludePatterns` | `['/login', '/auth']` | URL patterns to exclude |
| `contentSelector` | `'main, article, .content, #content'` | CSS selector for main content |
| `maxLinks` | `30` | Maximum pages to scrape |
| `singleFile` | `false` | Combine all pages into one markdown file |
| `concurrency` | `3` | Number of simultaneous requests |

## Examples

### API Documentation
```bash
# Scrape REST API docs
node scrape.js https://docs.stripe.com/api

# Creates: docs_stripe_com/api_*.md files
```

### Technical Guides
```javascript
await scrape.run('https://nextjs.org/docs', {
  includePatterns: ['/docs'],
  contentSelector: 'main',
  maxLinks: 25
});
```

### Single Combined File
```javascript
await scrape.run('https://docs.python.org/3/', {
  singleFile: true,
  maxLinks: 15
});
```

## Output Structure

The scraper creates organized markdown files:

```
website_name/
â”œâ”€â”€ docs_getting-started.md
â”œâ”€â”€ api_authentication.md
â”œâ”€â”€ reference_endpoints.md
â””â”€â”€ guides_quickstart.md
```

Each file contains:
- Clean markdown formatting
- Preserved code blocks and syntax highlighting  
- Working internal links
- Source URL for reference

## Markdown Conversion

The scraper intelligently converts HTML to markdown:

- `<h1>-<h6>` â†’ `# ## ### ####` headers
- `<strong>` â†’ `**bold text**`
- `<em>` â†’ `*italic text*`
- `<code>` â†’ `` `inline code` ``
- `<pre>` â†’ ``` code blocks ```
- `<a>` â†’ `[text](url)` links
- `<ul><li>` â†’ `- bullet lists`
- `<blockquote>` â†’ `> quoted text`

## Testing

Run the comprehensive test suite:

```bash
# Install test dependencies
npm install --save-dev tape

# Run tests
npm test

# Or directly
node test/scraper.test.js
```

Tests include:
- E2E scraping with real websites
- Markdown conversion accuracy
- File organization
- Error handling
- Performance benchmarks

## Error Handling

The scraper gracefully handles:
- Network timeouts and failures
- Invalid URLs and redirects
- Missing content selectors
- Rate limiting (with retry logic)
- Malformed HTML

Failed pages are logged but don't stop the scraping process.

## Best Practices

1. **Be Respectful**: Use reasonable concurrency (2-3) and delays
2. **Test First**: Try with `maxLinks: 5` before full scraping
3. **Check robots.txt**: Respect site scraping policies
4. **Use Specific Selectors**: Target main content areas for cleaner output

## Dependencies

- [axios](https://www.npmjs.com/package/axios) - HTTP requests
- [cheerio](https://www.npmjs.com/package/cheerio) - HTML parsing and DOM manipulation

## Use Cases

- ğŸ“š **Local Documentation** - Offline access to docs
- ğŸ” **Documentation Analysis** - Feed docs to LLMs for analysis
- ğŸ“‹ **Content Migration** - Convert old docs to markdown
- ğŸ—ï¸ **Documentation Backups** - Preserve important technical content
- ğŸ¤– **AI Training Data** - Prepare documentation for AI models

## Contributing

Feel free to submit issues and improvements! The scraper is designed to be simple, universal, and easily extensible.